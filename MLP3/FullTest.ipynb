{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ea793b",
   "metadata": {},
   "source": [
    "# MLP simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Datasets = []\n",
    "PREDICTORS = [\"PwmD\", \"PwmE\"]   \n",
    "TARGET = [\"Wd\", \"We\"]       \n",
    "\n",
    "for i in range(3):   \n",
    "    Dataset = pd.read_csv(f\"../Dados/Data{i + 1}.csv\")\n",
    "        \n",
    "    # Ajusta índice pelo tempo\n",
    "    Dataset.index = (np.arange(0, len(Dataset), 1).astype(float) * 0.07).round(5)\n",
    "    \n",
    "    Datasets.append(Dataset)\n",
    "    \n",
    "    print(f\"++++++++++++++++++++ Dataset {i+1} +++++++++++++++++++++++\")\n",
    "    print(Dataset.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd413ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NormDatasets = []\n",
    "\n",
    "SCALER = StandardScaler()\n",
    "OUT_SCALER = StandardScaler()\n",
    "\n",
    "TrainDataset = Datasets[0]\n",
    "TrainDataset[PREDICTORS] = SCALER.fit_transform(TrainDataset[PREDICTORS])\n",
    "TrainDataset[TARGET] = OUT_SCALER.fit_transform(TrainDataset[TARGET])\n",
    "NormDatasets.append(TrainDataset)\n",
    "\n",
    "for i in range(2):\n",
    "      CurrentTestDataset = Datasets[i + 1]\n",
    "      CurrentTestDataset[PREDICTORS] = SCALER.transform(CurrentTestDataset[PREDICTORS])\n",
    "      CurrentTestDataset[TARGET] = OUT_SCALER.transform(CurrentTestDataset[TARGET])\n",
    "      NormDatasets.append(CurrentTestDataset)\n",
    "      print(f\"++++++++++++++++++++ Dataset Normalizado {i+1} +++++++++++++++++++++++\")\n",
    "      print(NormDatasets[i].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(TrainDataset[PREDICTORS])\n",
    "y_train = np.array(TrainDataset[TARGET])\n",
    "\n",
    "x_val = np.array((NormDatasets[1])[PREDICTORS])\n",
    "y_val = np.array((NormDatasets[1])[TARGET])\n",
    "\n",
    "print(f\"Dimensão da entrada: {np.shape(x_train)}\")\n",
    "print(f\"Dimensão da saida: {np.shape(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9720f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def PlotHistory(history):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TITLES = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "def PlotOut(ax, title, target_name, y_true, y_pred):\n",
    "    time = (np.arange(0, len(y_pred), 1).astype(float) * 0.07).round(5)\n",
    "\n",
    "    ax.scatter(time, y_true, marker='o', s=12, label='Amostras Reais', alpha=0.7)\n",
    "    ax.scatter(time, y_pred, marker='x', s=12, label='Valores Preditos', alpha=0.7)\n",
    "    ax.set_title(f'{title} - {target_name}')\n",
    "    ax.set_xlabel('Tempo [s]')\n",
    "    ax.set_ylabel(target_name)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def EvalModel(model):\n",
    "    n_datasets = len(Datasets)\n",
    "    n_targets = len(TARGET)\n",
    "    fig, axs = plt.subplots(n_datasets, n_targets, figsize=(6 * n_targets, 4 * n_datasets))\n",
    "    \n",
    "    metrics = {name: {\"R2_train\": [], \"R2_test\": [], \"R2_val\": [],\n",
    "                      \"MSE_train\": [], \"MSE_test\": [], \"MSE_val\": [],} for name in TARGET}\n",
    "\n",
    "    for i, dataset in enumerate(NormDatasets):\n",
    "        x = np.array(dataset[PREDICTORS])\n",
    "        \n",
    "        # Desnormaliza saídas\n",
    "        y_true = OUT_SCALER.inverse_transform(dataset[TARGET])\n",
    "        y_pred = OUT_SCALER.inverse_transform(model.predict(x))\n",
    "\n",
    "        # Calcula métricas por saída\n",
    "        for j, name in enumerate(TARGET):\n",
    "            r2 = r2_score(y_true[:, j], y_pred[:, j])\n",
    "            mse = mean_squared_error(y_true[:, j], y_pred[:, j])\n",
    "            metrics[name][f\"R2_{TITLES[i]}\"].append(r2)\n",
    "            metrics[name][f\"MSE_{TITLES[i]}\"].append(mse)\n",
    "\n",
    "            print(f\"{name} | {TITLES[i]} -> R² = {r2:.4f}, MSE = {mse:.4e}\")\n",
    "            \n",
    "            # Seleciona o eixo correto (funciona mesmo com 1x1, 1x2 ou 3x2)\n",
    "            ax = axs[i][j] if n_datasets > 1 and n_targets > 1 else (\n",
    "                axs[j] if n_targets > 1 else axs[i] if n_datasets > 1 else axs\n",
    "            )\n",
    "            PlotOut(ax, TITLES[i], name, y_true[:, j], y_pred[:, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Retorna métricas médias para análise posterior\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f834d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import initializers\n",
    "\n",
    "# tamanho da entrada\n",
    "INPUT_SIZE = len(PREDICTORS)  \n",
    "OUTPUT_SIZE = len(TARGET) \n",
    "N_MODELS = 7  # número de inicializações\n",
    "excel_file = \"resultados.xlsx\"\n",
    "\n",
    "seeds = np.random.choice(range(1, 10000), size=N_MODELS, replace=False)\n",
    "neurons = [14, 16, 18, 19, 20]\n",
    "results = {}\n",
    "\n",
    "for n in neurons:\n",
    "    for i, s in enumerate(seeds):\n",
    "\n",
    "        initializer = initializers.RandomNormal(seed=int(s))\n",
    "\n",
    "        # cria modelo\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Input(shape=(INPUT_SIZE,)),\n",
    "            keras.layers.Dense(n, activation=\"tanh\"),\n",
    "            keras.layers.Dense(OUTPUT_SIZE, activation=\"linear\"),  \n",
    "        ])\n",
    "        \n",
    "        # salva pesos iniciais\n",
    "        w0 = model.get_weights()\n",
    "\n",
    "        # compila\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "        early_stopping_monitor = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=50,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # treina\n",
    "        history = model.fit(\n",
    "            x_train, \n",
    "            y_train, \n",
    "            epochs=500,\n",
    "            callbacks=[early_stopping_monitor],\n",
    "            validation_data=(x_val, y_val),\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # salva pesos finais\n",
    "        wf = model.get_weights()\n",
    "\n",
    "      # avalia o modelo\n",
    "        metrics = EvalModel(model)\n",
    "\n",
    "        row = {\n",
    "            \"model\": f\"model_{n}_{i}\",\n",
    "            \"Neurons\": n,\n",
    "            # \"Seed\": s,\n",
    "\n",
    "            # ---- Saída Wd ----\n",
    "            \"R2_Train_Wd\": metrics[\"Wd\"][\"R2_train\"],\n",
    "            \"MSE_Train_Wd\": metrics[\"Wd\"][\"MSE_train\"],\n",
    "            \"R2_Val_Wd\": metrics[\"Wd\"][\"R2_val\"],\n",
    "            \"MSE_Val_Wd\": metrics[\"Wd\"][\"MSE_val\"],\n",
    "            \"R2_Test_Wd\": metrics[\"Wd\"][\"R2_test\"],\n",
    "            \"MSE_Test_Wd\": metrics[\"Wd\"][\"MSE_test\"],\n",
    "\n",
    "            # ---- Saída We ----\n",
    "            \"R2_Train_We\": metrics[\"We\"][\"R2_train\"],\n",
    "            \"MSE_Train_We\": metrics[\"We\"][\"MSE_train\"],\n",
    "            \"R2_Val_We\": metrics[\"We\"][\"R2_val\"],\n",
    "            \"MSE_Val_We\": metrics[\"We\"][\"MSE_val\"],\n",
    "            \"R2_Test_We\": metrics[\"We\"][\"R2_test\"],\n",
    "            \"MSE_Test_We\": metrics[\"We\"][\"MSE_test\"],\n",
    "            \n",
    "             # ---- Pesos ----\n",
    "            \"W0\": str([w.round(4).tolist() for w in w0]),\n",
    "            \"Wf\": str([w.round(4).tolist() for w in wf]),\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame([row])\n",
    "\n",
    "        # salva/atualiza Excel incrementalmente\n",
    "        try:\n",
    "            # tenta abrir existente e adicionar linha\n",
    "            old = pd.read_excel(excel_file)\n",
    "            new_df = pd.concat([old, df], ignore_index=True)\n",
    "            new_df.to_excel(excel_file, index=False)\n",
    "        except FileNotFoundError:\n",
    "            # se não existir, cria arquivo novo\n",
    "            df.to_excel(excel_file, index=False)\n",
    "\n",
    "        print(f\"Modelo {i} treinado e salvo no Excel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
